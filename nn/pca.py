# -*- coding: utf-8 -*-
"""ANN7(PCA).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r_Ex8SAQKqtQxy0frpNJ4Oh4HJsYt9Lm
"""

#direct
import numpy as np
from sklearn.decomposition import PCA

# Generate some sample data
X = np.random.rand(100, 10)

# Instantiate a PCA object with the desired number of components
pca = PCA(n_components=3)

# Fit the PCA object to the data and transform the data
X_pca = pca.fit_transform(X)

# The transformed data now has reduced dimensions
print(X_pca.shape)

# not direct
import numpy as np
import matplotlib.pyplot as plt

# random data
X = np.random.rand(50, 2)

# plot the original data
plt.scatter(X[:, 0], X[:, 1], c='blue')
plt.title('Original Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# get mean of the data
mean = np.mean(X, axis=0)

# centering the data by subtracting the mean
X_centered = X - mean

# the covariance matrix of the centered data
covariance_matrix = np.cov(X_centered.T)

# eigenvalues and eigenvectors of the covariance matrix
eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)

# sort the eigenvectors in descending order of their corresponding eigenvalues
eigenvectors_sorted = eigenvectors[:, np.argsort(eigenvalues)[::-1]]

# choose the top k eigenvectors based on the desired number of components
k = 1
top_k_eigenvectors = eigenvectors_sorted[:, :k]

# transform the centered data using the top k eigenvectors
X_transformed = np.dot(X_centered, top_k_eigenvectors)

# plot the transformed data
plt.scatter(X_transformed[:, 0], np.zeros_like(X_transformed[:, 0]), c='red')
plt.title('Transformed Data')
plt.xlabel('PC 1')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

# generate some sample data
X = np.random.rand(200, 3)

# plot the original data
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c='#d30085')
ax.set_title('Original Data')
ax.set_xlabel('Feature 1')
ax.set_ylabel('Feature 2')
ax.set_zlabel('Feature 3')
plt.show()

# get mean of the data
mean = np.mean(X, axis=0)

# centering the data by subtracting the mean
X_centered = X - mean

# the covariance matrix of the centered data
covariance_matrix = np.cov(X_centered.T)

# eigenvalues and eigenvectors of the covariance matrix
eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)

# sort the eigenvectors in descending order of their corresponding eigenvalues
eigenvectors_sorted = eigenvectors[:, np.argsort(eigenvalues)[::-1]]

# choose the top k eigenvectors based on the desired number of components
k = 2
top_k_eigenvectors = eigenvectors_sorted[:, :k]

# transform the centered data using the top k eigenvectors
X_transformed = np.dot(X_centered, top_k_eigenvectors)

# total variance in the original data
total_variance = np.sum(np.var(X, axis=0))

# variance retained by the top k principal components
variance_retained = np.sum(np.var(X_transformed, axis=0))

# percentage of variance retained
percentage_retained = (variance_retained / total_variance) * 100

print(f'Percentage of variance retained: {percentage_retained:.2f}%')

# plot the transformed data
plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c='#642d8a')
plt.title('Transformed Data')
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.show()